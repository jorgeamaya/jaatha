% \VignetteIndexEntry{The Jaatha HowTo}
% \VignetteDepends{jaatha}
% \VignettePackage{jaatha}

\documentclass{article}

\usepackage{natbib}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}

\begin{document}

<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
jinfo <- packageDescription("jaatha")
load('startpoints.save')
@

\title{The Jaatha HowTo}
\author{\Sexpr{jinfo$Author}}
\date{Version \Sexpr{jinfo$Version}}
\maketitle

\section{Introduction}
\noindent
Jaatha is a fast composite likelihood method to estimate model parameters of the 
demographic history of (at the moment) two related species. To do so, it uses molecular
data from multiple individuals from both species.
This HowTo describes the method and gives an example of using its implementation
as \verb@R@ package \verb@jaatha@.

\section{A demographic model}
\noindent
Before we can apply Jaatha to estimate parameter, we first need to create a model
of the demographic history of the species. 

For now, assume that we know that our two species are closely related; hence they must 
have separated at a time in the past. There may be still gene flow between them, 
which we will refer to as migration from one population into the other. We suppose that 
mutations appear in the genome with a constant rate, which we don't know, and that 
recombination events occur frequently. Hence, we could propose the following, simple model:

\begin{center}
\includegraphics[width=60mm]{dm.png}
\end{center}

\noindent
The ancestral population spilt into our two populations $\tau$ time units ago, and afterwards
individuals migrated from one population to the other with a migration rate $m$. Mutations are
occurring with rate $\theta$ and recombination with a known rate $\rho$.

To specify the above demographic model in \verb@R@, we first need to load \verb@jaatha@.
<<Loading Jaatha>>=
library(jaatha)
@
We can now create an 'empty' model \verb@dm@ using the \verb@dm.createDemographicModel()@
function:
<<Creating a demographic model in R>>=
dm <- dm.createDemographicModel(sampleSizes=c(25,24),nLoci=100,
						seqLength=10^3)
@
The parameter \verb@sampleSizes@ here states we have sampled molecular data from $25$ 
individuals of the first population and $24$ individuals from the second one. The 
second argument \verb@nLoci@ states are using data from $100$ independent loci in the
genome while \verb@seqLength@ gives the (average) length of each loci\footnote{This 
is only used when a finite sites model is assumed or if recombination is included.}.

We can now successively add the other assumptions of our model:
<<Creating a demographic model in R>>=
dm <- dm.addSpeciationEvent(dm,.1,5)
dm <- dm.addSymmetricMigration(dm,.01,5)
dm <- dm.addMutation(dm,1,20)
dm <- dm.addRecombination(dm,fixed=5)
@
Here, the first parameter is always the demographic model to which we want to add
an assumption/feature, while the first number is the lowest possible value we assume
that our parameter could have and the second number is the highest possible value.
\begin{itemize}
\item The parameter for the \emph{speciation} event is the split time $\tau$, which
	states how many generations ago the split of the population has occurred.
	As usual in population genetics, its is measured in units of $4N_1$ 
	generations ago, where $N_1$ is the (diploid) effective population size of 
	the first population.
\item The parameter for the (symmetric) \emph{migration} is the scaled migration rate $M$, 
	which is given by $M=4N_1m$, where $m$ is the fraction of individuals of each 
	population which are replaced by immigrants from the other population each
	generation.
\item The \emph{mutation} parameter $\theta$ is scaled with $4N_1$ times the neutral mutations
	rate per locus.
\item Finally, the \emph{recombination} parameter $\rho$ is scaled with $4N_1$ times
	the probability of recombination between the ends of the locus per generation.
\end{itemize}

\noindent
Finally we can check your model using
<<Checking the model>>=
dm
@

Keep in mind that a `good' model -- which is one that approximates the real demographic
history but is also as simple as possible -- is crucial for getting meaningful
estimates in the end. Jaatha will always try to find the parameters that make the
model fit best to the real data. If the model does not fit to the real data at all,
also the estimates will not be related to the real situation.

\section{Theoretical Background}
\noindent
It is important to understand the key concepts behind Jaatha before we apply it. Like many
estimation methods that rely on simulations, Jaatha tries to find the parameters that fits
best\footnote{for Jaatha, the 'best' parameter combination is the one with the highest
composite likelihood} to your data by simulating artificial data for many different parameter
combinations. It uses a learning algorithm to determine how the different parameter
values influence the data and uses that knowledge to find the best parameter combination for 
your data. 

You can imagine Jaatha as a method that runs through the parameter space -- the space
of all possible parameter combinations, in our example a cube with borders from $0.1$ to $5$, 
$0.01$ to $5$ and $1$ to $20$ -- 



- Parameter space
- Blocks
- SumStats
- GLMs



\section{Importing Your Data}
<<Summary Statistics>>=
pars 	<- c(1,1,10) 
simSumStats <- dm.simSumStats(dm,pars)
simSumStats
@

\section{Running Jaatha}

\noindent
Jaatha is divided into two parts. First we find good starting
positions by simulating very coarsely across the entire parameter space. We call
this part \emph{initial search}. Afterwards a more thorough \emph{refined search}
is performed starting from the best positions of the first step.


<<Initialize>>=
j1 <- Jaatha.initialize(dm, simSumStats, resultFile='res.txt')
@
%startPoints <- Jaatha.initialSearch(j1,nSim=500,nBlocksPerPar=3,multiple=T)
%save(startPoints,file='startpoints.save')

\subsection{The Initial Search}

\noindent
For the initial search, we divide the parameter space into equally-sized blocks by dividing 
each of the $n$ parameters ranges into \verb@nBlocksPerPar@ intervals such that we obtain 
$($\verb@nBlocksPerPar@$)^n$ blocks. Within each block we simulate \verb@nSim@ data sets with 
-- on the logarithmic scale -- uniformly drawn parameter values within each block. To ensure 
a better sampling of the edges, we simulate in addition data sets for all corner points of 
each parameter block.

For these data sets we then fit the GLMs and estimate the parameter combination with the
maximal score. Each of the blocks provides a single best parameter combination.

In R, the initial search is performed with the command

% Don't run a complete jaatha search; would take tooo long
\vspace{0.5em}
\noindent
\verb@> startPoints <- Jaatha.initialSearch(j1,nSim=200,nBlocksPerPar=3)@
\vspace{-0.6em}

\noindent
which takes a while, and finally returns a list of $3^3$ start points sorted by score:
<<Initial Search>>=
Jaatha.printStartPoints(j1,startPoints)
@
Here, there is a big reduction in the scores after the first five blocks, and a smaller one 
after the first two. This is suggesting that we either use the first two or the first five 
blocks as starting positions for the refined search, depending on how much time we want to
spend. For now, we will just use the first two points.
<<<Pick points>>=
startPoints <- Jaatha.pickBestStartPoints(startPoints,2)
@



\subsection{The Refined Search}

Taking a set of best starting points chosen by 
their score we can then conduct a more thorough search to fit GLMs to the 
simulated $\widehat{\textbf{S}}$ in smaller regions of the parameter space and 
thus improving the fit of the GLMs and consequently the likelihood approximations. 
The score is proportional to the likelihood and is defined as equation without the
denumerator $s_i!$, since it does not change during the course of the Jaatha run.
In the following paragraphs we will explain in more detail the two phases and the variables
that can be specified by the user.

\begin{enumerate}
  \item Initial Search: Finding good starting positions\\
  \item Refined search: Getting the point estimate\\
  For $b \in \left\{1,\ldots, n_{RP}\right\}$ do:
      \begin{enumerate}
	\item 
    Around $\mathbf{\tilde{p}}_{b}$ we perform a \textit{Jaatha step}: We define a block 
    $\mathcal{B}_{\mathbf{\tilde{p}}_{b}}$ with middle point $\mathbf{\tilde{p}}_{b}$ 
    and side length $r$ in all 
    dimensions on the parameter range, simulate 
    $s_{main}$ data sets of $n_{loc}$ loci with uniformly chosen parameters from within this 
    block (corner points in addition), calculate $\widehat{\textbf{S}}$,
    fit GLMs as described above, and estimate a new optimal parameter 
    combination $\mathbf{\tilde{p}}_{b}^\prime$.  
    Now around $\mathbf{\tilde{p}}_{b}^\prime$ we run a Jaatha step 
    %$\mathcal{B}_{\mathbf{\tilde{p}}_{b}^\prime}$
    to find $\mathbf{\tilde{p}}_{b}^{\prime\prime}$. 
    For the GLM fitting to find $\mathbf{\tilde{p}}_{b}^{\prime\prime}$ we only 
    reuse simulations of previous blocks if 
    $\mathbf{\tilde{p}}_{b}^\prime$ falls within the 
    block, otherwise the simulations are deleted from memory. 
    Especially for the FSM runs this was necessary to reduce the amount of 
    memory usage.
    This procedure is iterated and the search stops
when the score of the new parameter combination failed to change over the last 
    $t_{stop}$ steps by at least $\epsilon$. The maximum number of steps can be 
    specified as another stopping criterion ($t_{max}$) which was necessary in 
    particular when $\epsilon$ was small such
    that the score did not seem to converge. 
    Throughout this phase we keep a list of $n_B$ 
    parameter combinations with the highest scores ($\mathcal{L}$). 

    To avoid being trapped in local maxima there is an 
    option to weigh simulations of previous blocks with $w \in [0,1]$.
    Each time simulations of a block are kept, we multiply the weight of these simulations 
    in the GLM fitting by $w$, such that if $w=1$ all simulation 
    results have the same contribution in each step.

	\item Evaluation of the parameter estimates in $\mathcal{L}$:\\
    After the phase 2 (a) has finished, the parameter combinations which were stored in 
    $\mathcal{L}$ will be used to simulate $s_{final}$ independent
    simulations for each of them to calculate the (composite) likelihood of each parameter 
    combination (using eqn.~\ref{PoissonApprox}). 
    The combination with the highest likelihood will then be reported
    as the result for $b$.
        \end{enumerate}
Since we start the detailed search for each of the $n_{RP}$ refine points,
Jaatha will report $n_{RP}$ parameter combinations in total. 
The Jaatha results in the following always represent the
parameter combination with the highest likelihood.
\end{enumerate}

Another option that can be set by the user is $ext_\theta$, which specifies whether
$\theta$ is excluded from the parameter range from which the random values are 
chosen for the simulations. If this option is set, for the simulations $\theta$
is fixed to the value of 5, which reduces the dimension of block $\mathcal{B}$ by one
while the other parameters are calculated as described above.
$\theta$ is then estimated separately of the other parameters with 
\begin{equation} \label{eqn:extTheta}
\widehat{\theta}_\mathcal{B} = \frac{\sum\limits_{i=0}^{n_{SS}}s_i}
{\sum\limits_{j=0}^{n_{SS}}\widehat{\lambda_i}
(\mathbf{\hat{p}}_\mathcal{B})/(5\cdot n_{loc})},
\end{equation}
where the parameter combination of the block center of $\mathcal{B}$ is denoted by
$\mathbf{\hat{p}}_\mathcal{B}$. 

\end{document}
