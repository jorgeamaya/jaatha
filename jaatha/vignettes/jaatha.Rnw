% \VignetteIndexEntry{The Jaatha HowTo}
% \VignetteDepends{jaatha}
% \VignettePackage{jaatha}

\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}

\usepackage{natbib}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{hyperref}

<<foo,include=FALSE,echo=FALSE>>=
options(keep.source = TRUE, width = 60)
jinfo <- packageDescription("jaatha")
@

\hypersetup{
	pdftitle={The Jaatha HowTo \Sexpr{jinfo$Version}},
	pdfauthor={\Sexpr{jinfo$Author}},
	colorlinks=true,
	linkcolor=black,          % color of internal links
	citecolor=black,        % color of links to bibliography
	filecolor=black,      % color of file links
	urlcolor=blue           % color of external links
}

\begin{document}


\title{The Jaatha HowTo}
\author{\Sexpr{jinfo$Author}}
\date{Version \Sexpr{jinfo$Version}}
\maketitle

\section{Introduction}
\noindent
Jaatha is a fast composite likelihood method to estimate model parameters of the
evolutionary history of (at the moment) two related species or populations.
To do so, it uses SNP data from multiple individuals from both species and -- optionally but
highly recommended -- one or more outgroup sequences.
This HowTo describes the method and gives an example of using its implementation
as an \verb@R@ package \verb@jaatha@.

The package itself can be obtained from CRAN
using
<<install, eval=FALSE>>=
install.packages('jaatha')
@ 
or downloaded from
\url{http://evol.bio.lmu.de/_statgen/software/jaatha}. Jaatha runs on all
operating systems supported by \verb@R@ -- namely Windows, OS X (Mac) and Linux
-- with one minor restriction: On Windows the parallelization is currently not
working.

A more detailed description of the algorithm can be found in
\cite{naduvilezhath_jaatha:_2011}. 
Further information about the
\verb@R@ functions used in this document can be obtained by calling 
\verb@help()@ with the functions name as argument.

Please cite the above paper when using Jaatha in a publication.


\section{A demographic model}
\noindent
Before we can apply Jaatha to estimate parameters, we first need to create a model
of the evolutionary history of the two species. Jaatha cannot account for the
effects of selection, hence we assume a neutral evolution. It can estimate the 
effects that either ``demographic'' events -- like an expansion of the
size of one population or migration between the two populations -- or molecular events --
like mutation or recombination -- have on the genome of the populations. To
emphasize that we can not include selection, we refer to a scenario of the
evolution of the two species under such events as a ``demographic model''.

For now, assume that we know that our two species are closely related; hence they must 
have separated at a certain time in the past. There may still be gene flow
ongoing between them 
to which we will refer to as migration from one population into the other. 
Hence, we could propose the simple demographic model described in Figure~\ref{fig:dm}.

\begin{figure}
\begin{center}
\includegraphics[width=60mm]{img/dm.png}
\end{center}
\caption{
A simple demographic model: The ancestral population splits into two populations $\tau$ time 
units ago, and afterwards individuals migrated from one population to the other with a 
migration rate $M$. Mutations are occurring with rate $\theta$ and recombination with a 
known rate $\rho$.}
\label{fig:dm}
\end{figure}

To specify that model in \verb@R@, we first need to load \verb@jaatha@.
<<Loading Jaatha>>=
library(jaatha)
@
We can now create an 'empty' demographic model \verb@dm@ using the\\ 
\verb@dm.createDemographicModel()@ function:

<<Creating a demographic model in R>>=
dm <- dm.createDemographicModel(sample.sizes=c(25,24), 
                                loci.num=100,
                                seq.length=1000)
@

The parameter \verb@sample.sizes@ here corresponds to the number of individuals we have
sampled from the first population and second population respectively. The 
second argument \verb@loci.num@ states that we are using data from $100$ independent loci
while \verb@seq.length@ gives the (average) length of each loci\footnote{This 
is only used when a finite sites model is assumed or if intra-locus recombination is included.}.

We can now successively add the other assumptions of our model:
<<Creating a demographic model in R>>=
dm <- dm.addSpeciationEvent(dm, .1, 5, new.time.point.name='tau')
dm <- dm.addSymmetricMigration(dm, .01, 5, new.par.name='m')
dm <- dm.addMutation(dm, 1, 20, new.par.name='theta')
dm <- dm.addRecombination(dm, fixed=20)
@
Here, the first parameter is always the demographic model to which we want to add
an assumption/feature. The two following numbers represent the range for the parameter. 
The lower border has to be greater the zero, as we are using a logarithmic transformation
of the parameter space. The parameters are scaled as in the popular simulation program
\verb@ms@ (\cite{hudson_generating_2002}) that we use for simulations:
\begin{itemize}
\item The parameter for the \emph{speciation} event is the split time $\tau$, which
	states how many generations ago the split of the population has occurred.
	As usual in population genetics, its is measured in units of $4N_1$ 
	generations ago, where $N_1$ is the (diploid) effective population size of 
	the first population.
\item The parameter for the (symmetric) \emph{migration} is the scaled migration rate $M$, 
	which is given by $M=4N_1m$, where $m$ is the fraction of individuals of each 
	population which are replaced by immigrants from the other population each
	generation.
\item The \emph{mutation} parameter $\theta$ is $4N_1$ times the neutral mutations
	rate per locus.
\item Finally, the \emph{recombination} parameter $\rho$ is $4N_1$ times
	the probability of recombination between the ends of the locus per generation.
\end{itemize}

\noindent
%Finally we can check your model using
%<<Checking the model>>=
%dm
%@

Keep in mind that a `good' model -- which is one that approximates the real demographic
history but is also as simple as possible -- is crucial for obtaining meaningful
estimates in the end. Jaatha will always try to find the parameters that make the
model fit best to your data. If the model does not fit to the data at all,
Jaatha will still return estimates, but they will not be meaningful.

\section{Theoretical Background}
\noindent
It is important to understand the key concepts behind Jaatha before we can apply it. Like many
estimation methods that rely on simulations, Jaatha tries to find the parameters that best fits
\footnote{for Jaatha, the 'best' parameter combination is the one with the highest
composite likelihood} to your data by simulating artificial data for many different parameter
combinations. It uses a learning algorithm to determine how the different parameter
values influence the simulated data and uses that knowledge to find the best parameter combination for 
your data. 

You can imagine Jaatha as a method that runs through the parameter space -- the space
of all possible parameter combinations, in our example a cube with borders from $0.1$ to $5$, 
$0.01$ to $5$ and $1$ to $20$ -- simulating in a small part of the parameter space
around the current position (we call this area a \emph{block}). It then searches the new maximum
of the current blocks and moves to it, builds a new block around it and so on. The search finally 
stops when the likelihood cannot be improved anymore or a maximal number of steps has been reached.

To compare the simulated data to the real one, Jaatha uses \emph{summary statistics} of the data.
As default, it calculates the Joint Site Frequency Spectrum (JSFS) of the data and further summarizes
it by evaluating different sums over the JSFS. Please refer to \cite{naduvilezhath_jaatha:_2011} for a detailed
description.


\section{Importing Your Data}
\noindent
To run Jaatha, you need to calculate the JSFS of your data. To do so, you can
use the function \verb@calculateJsfs()@. This function accepts data imported
with the \verb@read.dna()@ function from the package \emph{ape}. It assumes that you provide
a joined, aligned data set with multiple samples from two populations and -- optional 
but highly recommended -- one or more outgroup sequences. Additionally, you must
provide the numbers of the sequences in the dataset that belong to population
one, population two and the outgroup respectively.

Please consult the documentation of \emph{ape} in order to get more information
about \verb@read.dna()@. As example, the import of the data could look like this:  

<<Load Fasta files>>=
library(ape)
# The path to the data
sample.file <- system.file('example_fasta_files/sample.fasta', 
                           package='jaatha')

# Reading the data
sample.data <- read.dna(sample.file, format='fasta', 
                        as.character=TRUE)

# Calculating the JSFS
sample.jsfs <- calculateJsfs(sample.data, 
                             pop1.rows=3:7, 
                             pop2.row=8:12,
                             outgroup.row=1:2)
sample.jsfs
@

\vspace{1em}
\noindent
For the purpose of this HowTo, we will use a simulated JSFS, for which we 
know the real parameter and hence see how well Jaatha performs:

<<Summary Statistics>>=
# Real parameters: M = 1, tau = 1 and theta = 10
real.pars <- c(1,1,10) 

# Simulate a JSFS with this parameters
jsfs <- dm.simSumStats(dm, real.pars)

# Print the upper left part of the JSFS
jsfs[1:10, 1:10]
@

\section{Running Jaatha}

\noindent
Jaatha is divided into two parts. First we find good starting
positions by simulating very coarsely across the entire parameter space. We call
this part \emph{initial search}. Afterwards a more thorough \emph{refined search}
is performed starting from the best positions of the first step. Before, we need
to set some options like our demographic model and the summary statistics of the 
real data:

<<Initialize>>=
jaatha <- Jaatha.initialize(dm, jsfs=jsfs)
@

<<Initialize-with-seed, include=FALSE, echo=FALSE>>=
jaatha <- Jaatha.initialize(dm, jsfs=jsfs, seed=50)
@

<<SaveJaatha, include=FALSE, echo=FALSE>>=
save(jaatha, file='cache/saved_objects/jaatha.Rsave')
@

\noindent
For more options refer to \Verb@?Jaatha.initialize@ or the Jaatha manual.

\subsection{The Initial Search}

\noindent
For the initial search, we divide the parameter space into equally-sized blocks by dividing 
each of the $n$ parameters ranges into \verb@nBlocksPerPar@ intervals such that we obtain 
$($\verb@nBlocksPerPar@$)^n$ blocks. Within each block we simulate \verb@nSim@ data sets with 
-- on a logarithmic scale -- uniformly drawn parameter values within each block. To ensure 
a better sampling of the edges, we simulate in addition data sets for all corner points of 
each parameter block.

For these data sets we then fit the GLMs and estimate the parameter combination with the
maximal score. Each of the blocks provides a single best parameter combination.

In R, the initial search is performed with the command

% Don't run a complete jaatha search; would take tooo long
\input{cache/initialSearch}

<<LoadStartPoints, include=FALSE, echo=FALSE>>=
load('cache/saved_objects/start-points.Rsave')
@

\noindent
which takes a while, and finally returns a list of start points sorted by score:
<<PrintStartPoints>>=
Jaatha.printStartPoints(jaatha, start.points)
@
Here, there is a big reduction in the scores after the first five blocks, and a smaller one 
after the first two. This is suggesting that we either use the first two or the first five 
blocks as starting positions for the refined search, depending on how much time we want to
spend. For now, we will just use the first two points.
<<Pick points>>=
picked.points <- Jaatha.pickBestStartPoints(start.points, 2)
@

<<SavePickedPoints, include=FALSE, echo=FALSE>>=
save(picked.points, file='cache/saved_objects/picked-points.Rsave')
@

\subsection{The Refined Search}

<<LoadFinalResults, include=FALSE, echo=FALSE>>=
load('cache/saved_objects/jaatha-complete.Rsave')
@

\noindent
Now we can conduct the more thorough refined search described above to improve the
likelihood approximations. 

\input{cache/refineSearch}

\noindent
Hence we perform two independent searches, starting in the two \verb@startPoints@ we choose before 
and according to the general options we choose during initialization, which are now stored in the 
\verb@jaatha@ object. In each step, we build a block with \verb@halfBlockSize@ in each direction
(on a logarithmic scale) and perform simulations for \verb@nSim@ random parameter combinations within
this block (plus one for very corner). We use this information to estimate the composite maximum 
likelihood parameters within this block and take this value as new starting position for the next
step. 

The algorithm stops when the score has not changed more than \verb@epsilon@ for five consecutive 
steps or step \verb@nMaxStep@ is reached. To avoid getting stuck in local maxima, 
the \verb@weight@ option decreases the weight of simulations of previous blocks.

Finally the likelihoods for the best ten parameter combinations are approximated using \verb@nFinalSim@
simulations. These are values printed at the end of the search. This matrix can also be accessed via

<<printLT>>=
likelihoods <- Jaatha.printLikelihoods(jaatha)
print(likelihoods[1:3, ])
@

%\begin{figure}
%  \begin{center}
%<<<label=fig1,fig=TRUE,echo=FALSE>>=
%<<printLT>>
%@
%  \end{center}
%  \caption{Likelihoods sorted by value}
%  \label{fig:one}
%\end{figure}


\bibliographystyle{plainnat}
\bibliography{jaatha}

\end{document}
